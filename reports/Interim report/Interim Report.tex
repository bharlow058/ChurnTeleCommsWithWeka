\documentclass[a4paper,11pt]{article}
\pagestyle{plain}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{amsthm}
\usepackage{url}
\usepackage{pgfplots}
\usepackage{color}
\usepackage{comment}
\usepackage[hmargin=1cm,vmargin=1cm]{geometry}

% TÃ¶fluredding
% allows for temporary adjustment of side margins
\usepackage{chngpage}
% provides filler text
\usepackage{lipsum}
\usepackage{multirow}

% Colour table cells
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\setlength{\parindent}{2cm}
\usepackage{float}
\restylefloat{table}

\title{Data Mining and Exploration\\Interim Report}
\author{Thorvaldur Helgason (s1237131) \\
Daniel Stanoescu (s0838600) \\
Maria Alexandra Alecu (STUDENT NUMBER)}
\pagestyle{fancy}

\begin{document}
    
\maketitle



% CONTENT
\section*{What we have done so far:}
\begin{itemize}
	\item Pre-processing:
	\begin{itemize}
		\item Replaced missing values with both zeros and mean values.
		\item Converted the dataset to a binary bag-of-features.
	\end{itemize}
	\item Familiarized ourselves with Naive Bayes, SVMs, and Decision Trees.  
\end{itemize}

The data we have chosen for this project is the Orange telecom customer behaviour dataset. We have been provided with a labelled training set for the data focusing on predicting the customer churn(switch providers), appetancy(tendancy to buy new products) or purcahse upgrades. Given the proposed dataset, we are looking at solving a classifcation problem.  To respect customer privacy, both the order of the customers as well as the variables in the dataset have been shuffled.

After inspecting the data, we have identified that i contains both numerical and categorical values. We R to explore as well as preprocess the data. We began with preprocessing the numerical data. We replaced all the entries that were empty with zero. Secondly we replaced the zero entries with the mean of the values of each variable in the dataset. In the case of the categorical data, we changed the empty strings to Not Available. We then converted all the strings to numeric values so we can handle data operations easier.

We began experimenting with a series of algorithms in order to figure out which ones would perform better. We looked at employing SVMs but we quickly abandoned the idea as they would require a higher computational requirement than other methods.

\subsection*{Decision trees - what we have done so far}
	We have implemented decision trees using the package "rpart" from R and tested it on the \textbf{pre-processed} data, using the first \textbf{50 features} of the data set. \\
	As an \textbf{evaluation} of this classifier, we have used the following methods:
	\begin{itemize}
		\item 2-fold cross validation
		\item the "printcp" function available in the "rpart" package. This function gives the error from 10-fold cross validation performed on the data set, using different values for the number of nodes in the decision tree. 
	\end{itemize}
	The \textbf{results} we obtained are as follows:
	\begin{itemize}
		\item With 2-fold cross validation, we obtained a very good accuracy, namely of 91%.
		\item The results from the "printcp" function seem to contradict this result, in the sense that this function shows that no matter how many nodes we include in our decision tree, there is no decision tree that fits the data.
	\end{itemize}
	
	
	
	

\section*{What we plan on doing:}
\begin{itemize}
	\item See which pre-processing techniques and features work best for different classifiers.
	\item Familiarize ourselves with more classifiers.
	\item Compare the performance of the classifiers with the criteria described below.
\end{itemize}

We plan experiment with a linear classification method, more exactly, employing logistic regression methods. If there is time, a look into a mixture of gaussians to model the date will be attempted. 

\subsection*{Decision trees - what we plan on doing}
 	From the \textbf{data set} point of view, our goals are the following:
 	\begin{itemize}
 		\item  use the same functions on the \textbf{unprocessed} data (since decision trees can handle missing data and categorical data)
 		\item use \textbf{all the features} of the data set
 		\item perform cross-validation using \textbf{more folds}.
 	\end{itemize}
In what concerns the \textbf{methods} we want to use, our main goal is to use the classifiers we obtained at the previous step in \textbf{ensemble} methods, such as \textbf{random forests}, \textbf{bagging} and \textbf{boosting}. We plan to use the built-in function in R, such as "cforest" and also experiment with the same functions in Weka.





\section*{What comparisons we want to run:}

\begin{itemize}
	\item Split data up randomly: 80\% of instances will be the training set and 20\% the test set.
	\item Perform 5-fold cross-validation on the training set and do final evaluation on the test set.
	\item For each classifier we store their accuracy, confusion matrix, ROC curve and AUC.
\end{itemize}

\end{document}