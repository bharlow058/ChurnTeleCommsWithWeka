\documentclass[a4paper,11pt]{article}
\pagestyle{plain}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{amsthm}
\usepackage{url}
\usepackage{pgfplots}
\usepackage{color}
\usepackage{comment}
\usepackage[hmargin=1.5cm,vmargin=1.5cm]{geometry}

% TÃ¶fluredding
% allows for temporary adjustment of side margins
\usepackage{chngpage}
% provides filler text
\usepackage{lipsum}
\usepackage{multirow}

\newenvironment{myindent}[1]%
 {\begin{list}{}%
         {\setlength{\leftmargin}{#1}}%
         \item[]%
 }
 {\end{list}}

\usepackage{float}
\restylefloat{table}

\title{Data Mining and Exploration\\Interim Report}
\author{Thorvaldur Helgason (s1237131) \\
Daniel Stanoescu (s0838600) \\
Maria Alexandra Alecu (s1255964)}
\pagestyle{fancy}

\begin{document}
    
\maketitle

% CONTENT
\section*{What we have done so far:}
\subsection*{Data}
The data we have chosen for this project is the Orange telecom customer marketing database. We have been provided with a labelled training set and unlabeled test set for the data focusing on predicting the following three target values: customer churn (switch providers), appetancy (tendancy to buy new products) or purchase upgrades. Given the proposed dataset, we are aiming at solving a classification problem. To respect customer privacy, both the order of the customers as well as the variables in the dataset have been shuffled.

There are two datasets for this task, one small with 230 features and another big with 15000 features. We have decided to work solely on the small dataset to begin with because of the lack of proper computational resources, but we might try working with the big dataset if there will be time. Also, we are only going to be using the training data and labels because we do not have labels for the test data and can therefore not use it for evaluation purposes. Instead we are splitting the labeled training set up so 80\% of it will be used for training our classifier and 20\% of the date to evaluate it. We also make sure that the split is balanced in terms of classes.

After inspecting the data, we have identified that it contains both numerical and categorical values. We used R to explore as well as preprocess the data. We began with preprocessing the numerical data. We replaced all the entries that were empty with zero. Secondly we replaced the zero entries with the mean of the values of each variable in the dataset. In the case of the categorical data, we changed the empty strings to Not Available. We then converted all the strings to numeric values so we could handle data operations easier. Additionally, we created a binary bag-of-features dataset to see if it would work better with the Naive Bayes classifier. 


\subsection*{Classifiers}
We began experimenting with a series of algorithms in order to figure out which ones would perform better. We looked at employing SVMs but we quickly abandoned the idea as they would require a higher computational requirement than other methods. We have also familiarized ourselves with Naive Bayes and Decision Trees.

We are looking at Naive Bayes because that was used as a basis in the competition and we want to create our own baseline for our modified dataset. So far, it looks like Naive Bayes is performing better on the raw and binary datasets than the ones were missing values are replaced, although the overall performance is pretty poor (AUC $\sim$ 0.5).


\subsubsection*{Decision trees - what we have done so far}
	We have implemented decision trees using the package "rpart" from R and tested it on the \textbf{pre-processed} data, using the first \textbf{50 features} of the data set. \\
	As an \textbf{evaluation} of this classifier, we have used the following methods:
	\begin{itemize}
		\item 2-fold cross validation
		\item the "printcp" function available in the "rpart" package. This function gives the error from 10-fold cross validation performed on the data set, using different values for the number of nodes in the decision tree. 
	\end{itemize}
	The \textbf{results} we obtained are as follows:
	\begin{itemize}
		\item With 2-fold cross validation, we obtained a very good accuracy, namely of 91%.
		\item The results from the "printcp" function seem to contradict this result, in the sense that this function shows that no matter how many nodes we include in our decision tree, there is no decision tree that fits the data.
	\end{itemize}
	
	
	
	

\section*{What we plan on doing:}
\begin{itemize}
	\item See which pre-processing techniques and features work best for different classifiers.
	\item Familiarize ourselves with more classifiers.
	\item Compare the performance of the classifiers with the criteria described below.
\end{itemize}

We plan experiment with a linear classification method, more exactly, employing logistic regression methods. If there is time, a look into a mixture of gaussians to model the date will be attempted. 

\subsection*{Decision trees - what we plan on doing}
 	From the \textbf{data set} point of view, our goals are the following:
 	\begin{itemize}
 		\item  use the same functions on the \textbf{unprocessed} data (since decision trees can handle missing data and categorical data)
 		\item use \textbf{all the features} of the data set
 		\item perform cross-validation using \textbf{more folds}.
 	\end{itemize}
In what concerns the \textbf{methods} we want to use, our main goal is to use the classifiers we obtained at the previous step in \textbf{ensemble} methods, such as \textbf{random forests}, \textbf{bagging} and \textbf{boosting}. We plan to use the built-in function in R, such as "cforest" and also experiment with the same functions in Weka.





\section*{What comparisons we want to run:}

\begin{itemize}
	\item Perform 5-fold cross-validation on the training set and do final evaluation on the test set.
	\item For each classifier we store their accuracy, confusion matrix, ROC curve and AUC.
\end{itemize}

\end{document}